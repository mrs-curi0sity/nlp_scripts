{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is glove 6B 50T\n",
    "# udemy nlp with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean distance\n",
    "def dist1(a,b):\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance\n",
    "def dist2(a,b):\n",
    "    return 1-a.dot(b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "dtype: int64\n",
      "0    1\n",
      "1    2\n",
      "dtype: int64\n",
      "euclidian distance 1.0\n",
      "cosine distance 0.05131670194948623\n"
     ]
    }
   ],
   "source": [
    "a = pd.Series([1,1])\n",
    "b = pd.Series([1,2])\n",
    "print(a)\n",
    "print(b)\n",
    "print(f'euclidian distance {dist1(a,b)}')\n",
    "print(f'cosine distance {dist2(a,b)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "1    2\n",
      "dtype: int64\n",
      "0    2\n",
      "1    4\n",
      "dtype: int64\n",
      "euclidian distance 2.0\n",
      "cosine distance 0.05131670194948623\n"
     ]
    }
   ],
   "source": [
    "# the cosine distance does not change when doubeling\n",
    "# cosine distance only takes into account angel (the smaller the higher the similarity. )\n",
    "# euclidian looks at absolute length\n",
    "a_double = a*2\n",
    "b_double = b*2\n",
    "print(a_double)\n",
    "print(b_double)\n",
    "print(f'euclidian distance {dist1(a_double,b_double)}')\n",
    "print(f'cosine distance {dist2(a_double,b_double)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, metric = dist2, 'cosine'\n",
    "#alternative: dist, metric = dist1, 'euclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "def load_word2vec(path = '../large_files/glove.6B/glove.6B.50d.txt'):\n",
    "    print(f'loading word embeddings word to vec from path {path}')\n",
    "    \n",
    "    word2vec = {}\n",
    "    embedding = []\n",
    "    index_to_word = []\n",
    "    \n",
    "    with open(path) as file:\n",
    "        num = 0\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            \n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            \n",
    "            word2vec[word] = vec\n",
    "            \n",
    "            embedding.append(vec)\n",
    "            index_to_word.append(word)\n",
    "            \n",
    "            num +=1\n",
    "            if num % 200000 == 0:\n",
    "                print(f'line number {num}')\n",
    "                print(line)\n",
    "    \n",
    "    embedding = pd.DataFrame(embedding, index=word2vec.keys())\n",
    "    num_words, num_dims = embedding.shape\n",
    "    \n",
    "    print(f'total number of entries found:  {num_words}. Dimension: {num_dims}')\n",
    "    return(word2vec, embedding, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1        2         3\n",
      "the  0.418000  0.249680 -0.41242  0.121700\n",
      ",    0.013441  0.236820 -0.16899  0.409510\n",
      ".    0.151640  0.301770 -0.16763  0.176840\n",
      "of   0.708530  0.570880 -0.47160  0.180480\n",
      "to   0.680470 -0.039263  0.30186 -0.177920\n",
      "and  0.268180  0.143460 -0.27877  0.016257\n",
      "in   0.330420  0.249950 -0.60874  0.109230\n",
      "a    0.217050  0.465150 -0.46757  0.100820\n",
      "<class 'pandas.core.series.Series'>\n",
      "the    0.782052\n",
      ",      0.569733\n",
      ".      0.487448\n",
      "of     1.053425\n",
      "to     0.686611\n",
      "and    0.611121\n",
      "in     0.938873\n",
      "a      0.820469\n",
      "dtype: float64\n",
      "(array([3]),)\n",
      "of\n",
      "<class 'str'>\n",
      "the    2\n",
      ",      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# scribble\n",
    "embedding_test = embedding.iloc[:8, :4]\n",
    "print(embedding_test)\n",
    "\n",
    "test_array = [0.1, 0.2, 0.3, 0.1]\n",
    "\n",
    "distances = embedding_test.apply(lambda x: dist1(x, test_array), axis=1)\n",
    "print(type(distances))\n",
    "\n",
    "print(distances)\n",
    "max_value = np.max(distances)\n",
    "print(np.where(distances == max_value))\n",
    "\n",
    "print(np.argmax(distances))\n",
    "print(type(np.argmax(distances)))\n",
    "print(np.argsort(distances)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# king - man = X - woman\n",
    "# <=> X = woman - man + king\n",
    "# I wanted to solve this on my own, but it does not converge\n",
    "def find_analogies_ma(word1, word2, word3, embedding):\n",
    "    \n",
    "    for word in [word1, word2, word3]:\n",
    "        if word.lower() not in embedding.index:\n",
    "            print(f'sorry, I do not have an embedding for {word}')\n",
    "            return ''\n",
    "        \n",
    "    king = embedding.loc[word1.lower()]\n",
    "    man = embedding.loc[word2.lower()]\n",
    "    woman = embedding.loc[word3.lower()]\n",
    "    searched_vector = woman - man + king\n",
    "    \n",
    "    print('computing distances')   \n",
    "    \n",
    "    distances = embedding.apply(lambda x: dist1(x, searched_vector), axis=1)\n",
    "    print(f'nearest words: {all_distances.sort_values()[:3]}')\n",
    "    \n",
    "    return all_distances.sort_values()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing distances\n",
      "nearest words: king     2.602624\n",
      "ruler    4.369554\n",
      "ii       4.400222\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "king    2.602624\n",
       "dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogies_ma('man', 'woman', 'king', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3, D, index_to_word):\n",
    "  for w in (w1, w2, w3):\n",
    "    if w not in word2vec:\n",
    "      print(\"%s not in dictionary\" % w)\n",
    "      return\n",
    "\n",
    "  king = word2vec[w1]\n",
    "  man = word2vec[w2]\n",
    "  woman = word2vec[w3]\n",
    "  v0 = king - man + woman\n",
    "\n",
    "  distances = pairwise_distances(v0.reshape(1, D), embedding, metric=metric).reshape(V)\n",
    "  idxs = distances.argsort()[:4]\n",
    "  for idx in idxs:\n",
    "    word = index_to_word[idx]\n",
    "    if word not in (w1, w2, w3): \n",
    "      best_word = word\n",
    "      break\n",
    "\n",
    "  print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(w,index_to_word, word2vec, D,  n=5 ):\n",
    "  if w not in word2vec:\n",
    "    print(\"%s not in dictionary:\" % w)\n",
    "    return\n",
    "\n",
    "  v = word2vec[w]\n",
    "  distances = pairwise_distances(v.reshape(1, D), embedding, metric=metric).reshape(V)\n",
    "  idxs = distances.argsort()[1:n+1]\n",
    "  print(\"neighbors of: %s\" % w)\n",
    "  for idx in idxs:\n",
    "    print(\"\\t%s\" % index_to_word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Main':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings word to vec from path ../large_files/glove.6B/glove.6B.50d.txt\n",
      "line number 200000\n",
      "soroca 0.049805 -0.92157 -0.78814 0.40001 0.73747 0.23841 1.0884 0.74811 0.35275 -0.092519 0.068657 -1.5069 0.71963 -0.62382 0.42556 -0.12594 0.12767 0.48246 0.747 0.95415 -0.61376 -0.41512 -0.04666 0.91667 -0.70671 0.71371 -0.09069 0.63914 -0.22234 -0.51031 -1.009 -0.93208 0.53891 -0.32442 0.060975 0.12915 -0.17024 0.29168 0.14596 0.18409 -0.16361 0.21103 -0.092295 -0.49671 -1.3887 0.29717 0.070581 -0.19783 -0.62638 0.25806\n",
      "\n",
      "line number 400000\n",
      "sandberger 0.072617 -0.51393 0.4728 -0.52202 -0.35534 0.34629 0.23211 0.23096 0.26694 0.41028 0.28031 0.14107 -0.30212 -0.21095 -0.10875 -0.33659 -0.46313 -0.40999 0.32764 0.47401 -0.43449 0.19959 -0.55808 -0.34077 0.078477 0.62823 0.17161 -0.34454 -0.2066 0.1323 -1.8076 -0.38851 0.37654 -0.50422 -0.012446 0.046182 0.70028 -0.010573 -0.83629 -0.24698 0.6888 -0.17986 -0.066569 -0.48044 -0.55946 -0.27594 0.056072 -0.18907 -0.59021 0.55559\n",
      "\n",
      "total number of entries found:  400000. Dimension: 50\n"
     ]
    }
   ],
   "source": [
    "glove_path = '../large_files/glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "#takes about 10 sec\n",
    "word2vec, embedding, index_to_word = load_word2vec(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.24968</td>\n",
       "      <td>-0.41242</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.34527</td>\n",
       "      <td>-0.044457</td>\n",
       "      <td>-0.49688</td>\n",
       "      <td>-0.17862</td>\n",
       "      <td>-0.00066</td>\n",
       "      <td>-0.6566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.29871</td>\n",
       "      <td>-0.15749</td>\n",
       "      <td>-0.34758</td>\n",
       "      <td>-0.045637</td>\n",
       "      <td>-0.44251</td>\n",
       "      <td>0.18785</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>-0.18411</td>\n",
       "      <td>-0.11514</td>\n",
       "      <td>-0.78581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3        4         5        6        7   \\\n",
       "the  0.418  0.24968 -0.41242  0.1217  0.34527 -0.044457 -0.49688 -0.17862   \n",
       "\n",
       "          8       9    ...          40       41       42        43       44  \\\n",
       "the -0.00066 -0.6566   ...    -0.29871 -0.15749 -0.34758 -0.045637 -0.44251   \n",
       "\n",
       "          45        46       47       48       49  \n",
       "the  0.18785  0.002785 -0.18411 -0.11514 -0.78581  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, D = embedding.shape\n",
    "print(embedding.shape)\n",
    "embedding.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motivation - man = satisfaction - woman\n",
      "success - man = recognition - woman\n",
      "fun - man = kids - woman\n",
      "love - woman = me - man\n",
      "love - man = mother - woman\n",
      "king - man = queen - woman\n",
      "money - man = paying - woman\n",
      "sex - man = sexual - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "december - november = july - june\n",
      "miami - florida = houston - texas\n",
      "einstein - scientist = matisse - painter\n",
      "rice - china = bacon - germany\n",
      "beer - germany = drink - japan\n",
      "tree - forest = devotees - flock\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = friend - wife\n",
      "man - woman = brother - friend\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = queen - princess\n"
     ]
    }
   ],
   "source": [
    "word1 = 'motivation'\n",
    "word2 = 'man'\n",
    "word3 = 'woman'\n",
    "#find_analogies_Lena(embedding, word1, word2, word3) # this is slow AF\n",
    "find_analogies(word1, word2, word3, D, index_to_word)\n",
    "find_analogies('success', 'man', 'woman',  D, index_to_word)\n",
    "find_analogies('fun', 'man', 'woman',  D, index_to_word)\n",
    "find_analogies('love', 'woman', 'man',  D, index_to_word)\n",
    "find_analogies('love', 'man', 'woman',  D, index_to_word)\n",
    "\n",
    "find_analogies('king', 'man', 'woman',  D, index_to_word)\n",
    "find_analogies('money', 'man', 'woman',  D, index_to_word)\n",
    "find_analogies('sex', 'man', 'woman',  D, index_to_word)\n",
    "find_analogies('france', 'paris', 'london',  D, index_to_word)\n",
    "find_analogies('france', 'paris', 'rome',  D, index_to_word)\n",
    "find_analogies('paris', 'france', 'italy',  D, index_to_word)\n",
    "find_analogies('december', 'november', 'june',  D, index_to_word)\n",
    "find_analogies('miami', 'florida', 'texas',  D, index_to_word)\n",
    "find_analogies('einstein', 'scientist', 'painter',  D, index_to_word)\n",
    "find_analogies('rice', 'china', 'germany',  D, index_to_word)\n",
    "find_analogies('beer', 'germany', 'japan',  D, index_to_word)\n",
    "\n",
    "find_analogies('tree', 'forest', 'flock',  D, index_to_word)\n",
    "\n",
    "find_analogies('man', 'woman', 'she',  D, index_to_word)\n",
    "find_analogies('man', 'woman', 'aunt',  D, index_to_word)\n",
    "find_analogies('man', 'woman', 'sister',  D, index_to_word)\n",
    "find_analogies('man', 'woman', 'wife',  D, index_to_word)\n",
    "find_analogies('man', 'woman', 'friend',  D, index_to_word)\n",
    "find_analogies('man', 'woman', 'actress',  D, index_to_word)\n",
    "find_analogies('man', 'woman', 'mother',  D, index_to_word)\n",
    "find_analogies('heir', 'heiress', 'princess',  D, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tprince\n",
      "\tqueen\n",
      "\tii\n",
      "\temperor\n",
      "\tson\n",
      "neighbors of: france\n",
      "\tfrench\n",
      "\tbelgium\n",
      "\tparis\n",
      "\tspain\n",
      "\tnetherlands\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\tchina\n",
      "\tkorea\n",
      "\ttokyo\n",
      "\ttaiwan\n",
      "neighbors of: einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysics\n",
      "\theisenberg\n",
      "\tfreud\n",
      "neighbors of: woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\ther\n",
      "\tboy\n",
      "neighbors of: man\n",
      "\twoman\n",
      "\tboy\n",
      "\tanother\n",
      "\told\n",
      "\tone\n",
      "neighbors of: nephew\n",
      "\tcousin\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tson\n",
      "\tuncle\n",
      "neighbors of: february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\taugust\n",
      "\tseptember\n",
      "neighbors of: success\n",
      "\tachieved\n",
      "\tsuccessful\n",
      "\tever\n",
      "\tthanks\n",
      "\tbest\n",
      "neighbors of: money\n",
      "\tcash\n",
      "\tpaying\n",
      "\tfunds\n",
      "\tpay\n",
      "\traise\n",
      "neighbors of: love\n",
      "\tdream\n",
      "\tlife\n",
      "\tdreams\n",
      "\tloves\n",
      "\tme\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors('king',index_to_word, word2vec, D)\n",
    "nearest_neighbors('france',index_to_word, word2vec, D)\n",
    "nearest_neighbors('japan',index_to_word, word2vec, D)\n",
    "nearest_neighbors('einstein',index_to_word, word2vec, D)\n",
    "nearest_neighbors('woman',index_to_word, word2vec, D)\n",
    "nearest_neighbors('man',index_to_word, word2vec, D)\n",
    "nearest_neighbors('nephew',index_to_word, word2vec, D)\n",
    "nearest_neighbors('february',index_to_word, word2vec, D)\n",
    "nearest_neighbors('success',index_to_word, word2vec, D)\n",
    "nearest_neighbors('money',index_to_word, word2vec, D)\n",
    "nearest_neighbors('love',index_to_word, word2vec, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
